{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c363291",
   "metadata": {},
   "source": [
    "# Chapter 6: Fine-Tuning and Customization\n",
    "\n",
    "## Why Fine-Tune Models?\n",
    "\n",
    "Fine-tuning involves adapting pre-trained models to specific tasks or domains. By doing this, we can:\n",
    "1. Enhance performance on specialized tasks (e.g., medical diagnosis, legal text analysis).\n",
    "2. Reduce training time by leveraging pre-trained weights.\n",
    "3. Avoid the need for vast computational resources for training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Examples\n",
    "\n",
    "### Example 1: Fine-Tuning GPT-J on a Custom Text Dataset\n",
    "\n",
    "#### Step 1: Load the Dataset\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a custom dataset (e.g., IMDB reviews)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Display the dataset structure\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "#### Step 2: Tokenization\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for GPT-J\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "```\n",
    "\n",
    "#### Step 3: Model Loading and Training\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "# Load GPT-J model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gptj-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### Example 2: Evaluating Model Performance\n",
    "```python\n",
    "# Generate text using the fine-tuned model\n",
    "inputs = tokenizer(\"The movie was\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "\n",
    "# Decode and display the generated text\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quiz\n",
    "\n",
    "1. What is the main advantage of fine-tuning pre-trained models?\n",
    "   - A. Reduced computational cost.\n",
    "   - B. Improved domain-specific performance.\n",
    "   - C. Both A and B.\n",
    "\n",
    "2. Which component is responsible for converting raw text into model-ready input?\n",
    "   - A. Tokenizer\n",
    "   - B. Trainer\n",
    "   - C. Dataset loader\n",
    "\n",
    "---\n",
    "\n",
    "### Answers:\n",
    "1. **C**: Both A and B.\n",
    "2. **A**: Tokenizer.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "### Task:\n",
    "1. Fine-tune a model on a domain-specific dataset of your choice (e.g., product reviews, news articles).\n",
    "2. Evaluate its performance using test prompts and compare it with the pre-trained model.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Solution:\n",
    "```python\n",
    "# Fine-tune on product reviews dataset (similar steps as above)\n",
    "custom_data = {\"text\": [\"The product was excellent!\", \"Terrible experience.\"]}\n",
    "\n",
    "# Tokenize, fine-tune, and generate text as shown in the examples above.\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
