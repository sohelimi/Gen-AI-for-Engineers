{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ef7673",
   "metadata": {},
   "source": [
    "# Chapter 8: End-to-End Example\n",
    "\n",
    "In this chapter, we will work through a case study to demonstrate the capabilities of Generative AI in text summarization. We will compare open-source and proprietary models in terms of performance, cost, and ease of use.\n",
    "\n",
    "---\n",
    "\n",
    "## Case Study: Text Summarization\n",
    "\n",
    "Text summarization aims to produce a concise and meaningful summary of a given text. It has applications in:\n",
    "- News articles.\n",
    "- Research paper summaries.\n",
    "- Email or document summarization.\n",
    "\n",
    "### Comparison of Models\n",
    "1. **Open-Source Model**: BLOOM (BigScience)\n",
    "   - Fully open and multilingual.\n",
    "   - High customization flexibility.\n",
    "2. **Proprietary Model**: OpenAI GPT (via API)\n",
    "   - State-of-the-art performance out-of-the-box.\n",
    "   - Usage-based pricing.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Examples\n",
    "\n",
    "### Example 1: Fine-Tuning BLOOM for Summarization\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    labels = tokenizer(examples[\"highlights\"], max_length=150, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load BLOOM model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### Example 2: Using OpenAI GPT for Summarization\n",
    "```python\n",
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "# Summarize text using OpenAI API\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=\"Summarize the following text:\n",
    "\n",
    "\" + \"Generative AI is revolutionizing many industries...\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Generated Summary:\", response.choices[0].text.strip())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Metrics\n",
    "| Metric        | Open-Source (BLOOM)         | Proprietary (OpenAI GPT) |\n",
    "|---------------|------------------------------|--------------------------|\n",
    "| **Cost**      | Free (compute costs only)   | Usage-based pricing      |\n",
    "| **Ease of Use** | Requires setup and fine-tuning | Ready-to-use API         |\n",
    "| **Performance**| High (with fine-tuning)    | State-of-the-art         |\n",
    "| **Customization**| Fully customizable         | Limited customization    |\n",
    "\n",
    "---\n",
    "\n",
    "## Quiz\n",
    "\n",
    "1. Which open-source model is used in this example for text summarization?\n",
    "   - A. GPT-3\n",
    "   - B. BLOOM\n",
    "   - C. BERT\n",
    "\n",
    "2. What is a key benefit of using proprietary models?\n",
    "   - A. No setup required.\n",
    "   - B. Fully customizable.\n",
    "   - C. Free to use.\n",
    "\n",
    "3. Name two advantages of fine-tuning an open-source model.\n",
    "\n",
    "---\n",
    "\n",
    "### Answers:\n",
    "1. **B**: BLOOM\n",
    "2. **A**: No setup required.\n",
    "3. **Advantages**:\n",
    "   - Improved performance for domain-specific tasks.\n",
    "   - Control over data and customization.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "### Task:\n",
    "1. Fine-tune BLOOM on a custom dataset (e.g., product reviews or blog posts).\n",
    "2. Compare the summaries generated by BLOOM and OpenAI GPT for the same input.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Solution:\n",
    "```python\n",
    "# Compare summaries for the same text input\n",
    "text = \"Generative AI is transforming industries such as healthcare, finance, and education...\"\n",
    "\n",
    "# BLOOM summary (using fine-tuned model)\n",
    "bloom_input = tokenizer(text, return_tensors=\"pt\")\n",
    "bloom_output = model.generate(bloom_input[\"input_ids\"], max_length=100)\n",
    "print(\"BLOOM Summary:\", tokenizer.decode(bloom_output[0], skip_special_tokens=True))\n",
    "\n",
    "# OpenAI GPT summary (API example shown above)\n",
    "print(\"OpenAI GPT Summary:\", response.choices[0].text.strip())\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
