{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97df624",
   "metadata": {},
   "source": [
    "# Chapter 2: Setting Up Your Environment\n",
    "\n",
    "In this chapter, we'll go through the process of setting up your environment to work with Generative AI models. This includes installing the required tools and libraries, and understanding how to run models on both CPU and GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setting Up Jupyter Notebooks\n",
    "\n",
    "Jupyter Notebooks are an excellent tool for interactive coding and documentation. Follow the steps below to set up Jupyter:\n",
    "1. Install Jupyter via Anaconda or pip:\n",
    "   ```bash\n",
    "   pip install notebook\n",
    "   ```\n",
    "2. Launch Jupyter Notebook:\n",
    "   ```bash\n",
    "   jupyter notebook\n",
    "   ```\n",
    "3. Access the interface through your browser.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Installation of Required Libraries\n",
    "\n",
    "To use Generative AI models, we need the following libraries:\n",
    "- **Python**: Programming language for our implementations.\n",
    "- **Hugging Face Transformers**: Library for accessing pre-trained models.\n",
    "- **PyTorch**: Deep learning framework for running models.\n",
    "- **TensorFlow**: Another popular deep learning framework.\n",
    "\n",
    "### Installations\n",
    "```bash\n",
    "# Install Python (if not already installed)\n",
    "sudo apt install python3 python3-pip\n",
    "\n",
    "# Install Hugging Face Transformers\n",
    "pip install transformers\n",
    "\n",
    "# Install PyTorch (use the appropriate command from https://pytorch.org/get-started/locally/)\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# Install TensorFlow\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Running Models on CPU and GPU\n",
    "\n",
    "### Advantages of Using GPU\n",
    "- **Parallel Processing**: GPUs are designed for handling multiple computations simultaneously.\n",
    "- **Speed**: Model inference and training are significantly faster on GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "### Detecting GPU Availability in Python\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Running on CPU.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Running a Simple Text-Generation Task on CPU and GPU\n",
    "Using Hugging Face's Transformers library, we'll perform text generation on both CPU and GPU to compare performance.\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Load a text generation model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"Generative AI is transforming the future of\"\n",
    "\n",
    "# Run on CPU\n",
    "start_time = time.time()\n",
    "print(\"Running on CPU...\")\n",
    "torch.set_default_tensor_type('torch.FloatTensor')  # Force CPU usage\n",
    "cpu_output = generator(prompt, max_length=50)\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "# Run on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    start_time = time.time()\n",
    "    print(\"Running on GPU...\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')  # Force GPU usage\n",
    "    gpu_output = generator(prompt, max_length=50)\n",
    "    gpu_time = time.time() - start_time\n",
    "\n",
    "    # Print results\n",
    "    print(\"GPU Output:\", gpu_output[0][\"generated_text\"])\n",
    "    print(f\"Time taken on GPU: {gpu_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "# Print CPU results\n",
    "print(\"CPU Output:\", cpu_output[0][\"generated_text\"])\n",
    "print(f\"Time taken on CPU: {cpu_time:.2f} seconds\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quiz\n",
    "\n",
    "1. Which library is used to access pre-trained models?\n",
    "   - A. PyTorch\n",
    "   - B. TensorFlow\n",
    "   - C. Hugging Face Transformers\n",
    "\n",
    "2. What is the primary advantage of running models on GPUs?\n",
    "   - A. Increased memory.\n",
    "   - B. Faster parallel processing.\n",
    "   - C. Easier debugging.\n",
    "\n",
    "3. Write the command to install PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### Answers:\n",
    "1. **C**: Hugging Face Transformers.\n",
    "2. **B**: Faster parallel processing.\n",
    "3. Command:\n",
    "   ```bash\n",
    "   pip install torch torchvision torchaudio\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "1. Modify the text generation code to measure inference time for a longer input prompt.\n",
    "2. Test the code on both CPU and GPU with a custom prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Solution:\n",
    "```python\n",
    "# Modify prompt length\n",
    "long_prompt = \"Generative AI is a revolutionary technology that has the potential to transform multiple industries, including\"\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "long_output = generator(long_prompt, max_length=100)\n",
    "time_taken = time.time() - start_time\n",
    "\n",
    "print(\"Generated Text:\", long_output[0][\"generated_text\"])\n",
    "print(f\"Inference Time: {time_taken:.2f} seconds\")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
