{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ae1af2",
   "metadata": {},
   "source": [
    "# Chapter 5: Multimodal Models\n",
    "\n",
    "## What Are Multimodal Models?\n",
    "\n",
    "Multimodal models are AI systems designed to process and generate data across multiple modalities, such as text, images, and audio. These models integrate different types of data to perform tasks that require understanding across modalities.\n",
    "\n",
    "### Examples:\n",
    "1. **DALL-E**: Generates images from textual descriptions.\n",
    "2. **CLIP**: Links images and text for tasks like image captioning and zero-shot classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept Sketch\n",
    "Below is a sketch illustrating how multimodal models combine text and images:\n",
    "\n",
    "![Multimodal Models](https://upload.wikimedia.org/wikipedia/commons/2/28/AI_multimodal_examples.svg)\n",
    "\n",
    "This diagram shows how models like CLIP and DALL-E connect images and text.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Examples\n",
    "\n",
    "### Example 1: Image Captioning with CLIP\n",
    "```python\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load an image and prepare inputs\n",
    "image = Image.open(\"example.jpg\")\n",
    "inputs = processor(text=[\"a cat\", \"a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "print(\"Probabilities:\", probs)\n",
    "```\n",
    "\n",
    "### Example 2: Guided Image Generation with Stable Diffusion and CLIP\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load Stable Diffusion and CLIP models\n",
    "sd_pipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Generate an image with Stable Diffusion\n",
    "prompt = \"A futuristic cityscape\"\n",
    "image = sd_pipeline(prompt).images[0]\n",
    "\n",
    "# Evaluate the generated image with CLIP\n",
    "inputs = clip_processor(text=[\"a futuristic cityscape\", \"a forest\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = clip_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "print(\"CLIP Evaluation Probabilities:\", probs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quiz\n",
    "\n",
    "1. Which model is commonly used for generating images from text?\n",
    "   - A. CLIP\n",
    "   - B. DALL-E\n",
    "   - C. GPT-3\n",
    "\n",
    "2. What is the main function of CLIP in multimodal tasks?\n",
    "   - A. Generate text from images.\n",
    "   - B. Link images and text for classification and captioning.\n",
    "   - C. Train generative models.\n",
    "\n",
    "---\n",
    "\n",
    "### Answers:\n",
    "1. **B**: DALL-E\n",
    "2. **B**: Link images and text for classification and captioning.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "### Task:\n",
    "1. Use CLIP to generate captions for a set of custom images.\n",
    "2. Compare the captions generated for images of different categories (e.g., animals, landscapes).\n",
    "\n",
    "---\n",
    "\n",
    "### Example Solution:\n",
    "```python\n",
    "from PIL import Image\n",
    "\n",
    "# Load custom images\n",
    "images = [\"cat.jpg\", \"forest.jpg\", \"car.jpg\"]\n",
    "\n",
    "# Generate captions for each image\n",
    "for img_path in images:\n",
    "    image = Image.open(img_path)\n",
    "    inputs = clip_processor(text=[\"a cat\", \"a forest\", \"a car\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    print(f\"Probabilities for {img_path}:\", probs)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
